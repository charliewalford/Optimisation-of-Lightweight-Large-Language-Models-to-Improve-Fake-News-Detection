{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Starting Run 1 of 5...\n",
      "‚úÖ Loaded dataset with 100 entries.\n",
      "‚ö†Ô∏è Run 1 Null Values: 0\n",
      "‚úÖ Run 1 Accuracy: 55.00%\n",
      "‚è≥ Run 1 Time: 93.28 minutes\n",
      "\n",
      "üîÑ Starting Run 2 of 5...\n",
      "‚úÖ Loaded dataset with 100 entries.\n",
      "‚ö†Ô∏è Run 2 Null Values: 0\n",
      "‚úÖ Run 2 Accuracy: 58.00%\n",
      "‚è≥ Run 2 Time: 90.20 minutes\n",
      "\n",
      "üîÑ Starting Run 3 of 5...\n",
      "‚úÖ Loaded dataset with 100 entries.\n",
      "‚ö†Ô∏è Run 3 Null Values: 0\n",
      "‚úÖ Run 3 Accuracy: 61.00%\n",
      "‚è≥ Run 3 Time: 94.20 minutes\n",
      "\n",
      "üîÑ Starting Run 4 of 5...\n",
      "‚úÖ Loaded dataset with 100 entries.\n",
      "‚ö†Ô∏è Run 4 Null Values: 0\n",
      "‚úÖ Run 4 Accuracy: 58.00%\n",
      "‚è≥ Run 4 Time: 91.26 minutes\n",
      "\n",
      "üîÑ Starting Run 5 of 5...\n",
      "‚úÖ Loaded dataset with 100 entries.\n",
      "‚ö†Ô∏è Run 5 Null Values: 0\n",
      "‚úÖ Run 5 Accuracy: 56.00%\n",
      "‚è≥ Run 5 Time: 90.83 minutes\n",
      "\n",
      "üìä **Final Summary After 5 Runs**\n",
      "‚úÖ **Average Accuracy:** 57.60%\n",
      "‚è≥ **Average Execution Time:** 91.95 minutes\n",
      "‚ö†Ô∏è **Average Null Values per Run:** 0.00\n",
      "‚úÖ **Example Output file saved as C:\\Users\\charl\\OneDrive\\Documents\\Dissertation\\Code\\NoTrainTest\\PolitiFact_Deepseek_r1_7B_Test.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ollama\n",
    "import re\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define the model\n",
    "model = \"deepseek-r1:7b\"\n",
    "\n",
    "# Number of runs\n",
    "num_runs = 5\n",
    "\n",
    "# Store accuracy, time, and null values for each run\n",
    "accuracy_list = []\n",
    "time_list = []\n",
    "null_values_list = []  # List to store null counts per run\n",
    "\n",
    "# Run the entire process 5 times\n",
    "for run in range(1, num_runs + 1):\n",
    "    print(f\"\\nüîÑ Starting Run {run} of {num_runs}...\")\n",
    "\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load dataset from JSONL file\n",
    "    with open(r\"Datasets\\PolitiFact_PromptCompletion_Test.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "        clean_dataset = [json.loads(line) for line in f]\n",
    "\n",
    "    # Limit to the first 100 entries\n",
    "    clean_dataset = clean_dataset[:100]\n",
    "\n",
    "    print(f\"‚úÖ Loaded dataset with {len(clean_dataset)} entries.\")\n",
    "\n",
    "    # Store actual labels & predictions\n",
    "    actual_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    # Total number of entries for progress tracking\n",
    "    total_entries = len(clean_dataset)\n",
    "\n",
    "    # Iterate through dataset and test deepseek-r1:7B\n",
    "    for i, entry in enumerate(clean_dataset):\n",
    "        prompt = entry[\"prompt\"]\n",
    "        # Create a prompt without revealing the label\n",
    "        prompt = f\"The claim: '{prompt}'\\nIs this claim True or False? No explanation is required.\"\n",
    "\n",
    "        actual_label = 1 if entry[\"completion\"].strip().lower() == \"true\" else 0\n",
    "\n",
    "        # Print progress every 10 entries\n",
    "        # if i % 10 == 0 or i == total_entries - 1:\n",
    "        #     print(f\"Processing entry {i + 1} of {total_entries}...\")\n",
    "\n",
    "        # Send the prompt to Ollama\n",
    "        response = ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        model_output = response['message']['content'].strip().lower()\n",
    "\n",
    "        # Print the response.\n",
    "        # print(f\"\\n output: {model_output}\\n\")\n",
    "\n",
    "        # Clean the model's output by removing LaTeX and special characters\n",
    "        cleaned_output = re.sub(r'[^a-zA-Z\\s{}]', '', model_output)\n",
    "\n",
    "        # Extract last occurrence of \"true\" or \"false\"\n",
    "        matches = re.findall(r'\\b(true|false)\\b', cleaned_output, re.IGNORECASE)\n",
    "\n",
    "        if matches:\n",
    "            last_label = matches[-1].lower()\n",
    "            predicted_labels.append(1 if last_label == \"true\" else 0)\n",
    "        else:\n",
    "            # print(f\"‚ö†Ô∏è No 'true' or 'false' found in response {i+1}: {model_output}\")\n",
    "            predicted_labels.append(None)  # Handle unexpected cases\n",
    "\n",
    "        actual_labels.append(actual_label)\n",
    "\n",
    "    # Count the number of `None` (null) values\n",
    "    null_count = predicted_labels.count(None)\n",
    "    null_values_list.append(null_count) \n",
    "\n",
    "    # Print how many null values were found in this run\n",
    "    print(f\"‚ö†Ô∏è Run {run} Null Values: {null_count}\")\n",
    "\n",
    "    # Remove None values (if any) for accuracy calculation\n",
    "    filtered_actual = [a for a, p in zip(actual_labels, predicted_labels) if p is not None]\n",
    "    filtered_predicted = [p for p in predicted_labels if p is not None]\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(filtered_actual, filtered_predicted)\n",
    "    print(f\"‚úÖ Run {run} Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "    # Store accuracy for averaging later\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    # End the timer\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate total elapsed time in minutes\n",
    "    elapsed_time = (end_time - start_time) / 60\n",
    "    print(f\"‚è≥ Run {run} Time: {elapsed_time:.2f} minutes\")\n",
    "\n",
    "    # Store time for averaging later\n",
    "    time_list.append(elapsed_time)\n",
    "\n",
    "# üéØ Calculate and Print Averages\n",
    "average_accuracy = sum(accuracy_list) / num_runs\n",
    "average_time = sum(time_list) / num_runs\n",
    "average_nulls = sum(null_values_list) / num_runs\n",
    "\n",
    "output_file = r\"NoTrainTest\\PolitiFact_Deepseek_r1_7B_Test.json\"\n",
    "# Save results back to a JSON file\n",
    "results = [\n",
    "    {\n",
    "        \"prompt\": entry[\"prompt\"],\n",
    "        \"actual_label\": actual_labels[i],\n",
    "        \"predicted_label\": predicted_labels[i]\n",
    "    }\n",
    "    for i, entry in enumerate(clean_dataset)\n",
    "]\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(results, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"\\nüìä **Final Summary After 5 Runs**\")\n",
    "print(f\"‚úÖ **Average Accuracy:** {average_accuracy:.2%}\")\n",
    "print(f\"‚è≥ **Average Execution Time:** {average_time:.2f} minutes\")\n",
    "print(f\"‚ö†Ô∏è **Average Null Values per Run:** {average_nulls:.2f}\")\n",
    "print(f\"‚úÖ **Example Output file saved as {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
